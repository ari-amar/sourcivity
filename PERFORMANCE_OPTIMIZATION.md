# Performance Optimization Guide

## âš¡ Quick Start - What's Already Implemented

### âœ… Completed Optimizations

1. **Parallel PDF Processing** (3-5x faster)
   - Location: `app/api/utils/pdf_scraper.py` line 232
   - All PDFs now downloaded and processed concurrently
   - Automatic - no configuration needed
   - Console shows: "âš¡ Parallel extraction completed in X.XXs"

2. **Backend Performance Tracking**
   - Location: `app/api/search/parts.py` lines 17-145
   - Tracks timing for each operation stage
   - Returns timing data in API response
   - Detailed console logging

3. **Frontend Performance Display**
   - Location: `components/SearchResults.tsx` lines 157-166
   - Shows search completion time above results
   - Displays network and processing breakdown
   - Console logs with ðŸ” (parts) and ðŸ­ (services) icons

4. **Type-Safe Timing Data**
   - Location: `lib/types.ts` lines 8-14
   - TypeScript interfaces for timing information
   - Consistent across frontend and backend

### ðŸŽ¯ Expected Performance Gains
- **PDF Processing**: 3-5x faster for 5+ PDFs (was: 25-75s â†’ now: 5-15s)
- **Overall Search**: 30-50% faster end-to-end
- **Better Visibility**: Real-time timing in UI and console

---

## Search Performance Measurement

### Overview
Performance timing has been added to track search speed and identify bottlenecks. Timing data is now collected at multiple stages:

1. **Frontend Timing** (`lib/searchApi.ts`)
   - Total request time
   - Network latency
   - JSON parsing time
   - Data transformation time

2. **Console Logging**
   - All searches log detailed timing to browser console
   - Parts Search: ðŸ” icon
   - Services Search: ðŸ­ icon

3. **UI Display** (`components/SearchResults.tsx`)
   - Total search time displayed above results
   - Breakdown of network and processing time
   - Updates in real-time after each search

### How to Monitor Performance

1. **Browser Console**
   ```javascript
   // Open DevTools Console (F12 or Cmd+Option+I)
   // Perform a search
   // Look for logs like:
   ðŸ” Parts Search Performance: {
     total: "2456.32ms",
     network: "2450.15ms",
     parse: "4.82ms",
     transform: "1.35ms",
     backend: { ... }
   }
   ```

2. **UI Indicators**
   - After search completes, timing appears above results
   - Format: "Search completed in X.XXs (Network: X.XXs, Processing: X.XXXs)"

3. **Network Tab**
   - Open DevTools Network tab
   - Filter by "Fetch/XHR"
   - Check timing for `/api/search/parts` and `/api/search/services`

---

## Optimization Strategies

### 1. Backend Optimization (Biggest Impact)

#### Current Bottlenecks
- AI API calls (Anthropic Claude)
- Search engine API calls (Exa)
- PDF processing and extraction
- Specification extraction from datasheets

#### Recommended Optimizations

**A. Caching Strategy**
```python
# Backend implementation (Python)
# Add Redis or in-memory cache for:
# - Search results (by query hash)
# - PDF extractions (by URL hash)
# - AI responses (by prompt hash)

from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def get_cached_search(query_hash):
    # Return cached results if available
    pass

# Cache TTL recommendations:
# - Search results: 1 hour
# - PDF extractions: 24 hours
# - Spec columns: 7 days
```

**B. Parallel Processing** âœ… IMPLEMENTED
```python
# Process multiple PDFs concurrently
import asyncio

async def process_parts_parallel(urls):
    tasks = [extract_specs_from_pdf(url) for url in urls]
    return await asyncio.gather(*tasks)

# Expected improvement: 3-5x faster for 5+ parts
```

**Implementation Status:**
- âœ… Implemented in `app/api/utils/pdf_scraper.py` (line 232)
- PDFs are now downloaded and processed in parallel using `asyncio.gather()`
- Progress messages show parallel processing status
- Timing information logged to console

**Usage:**
- Automatic - no configuration needed
- Works with any number of PDFs
- Falls back gracefully on errors
- Shows timing: "âš¡ Parallel extraction completed in X.XXs"

**C. Database Indexing**
```sql
-- Add indexes for common queries
CREATE INDEX idx_parts_query ON parts USING GIN (query_vector);
CREATE INDEX idx_parts_specs ON parts USING GIN (specs);
CREATE INDEX idx_cached_results ON search_cache (query_hash, created_at);
```

**D. AI Prompt Optimization**
- Reduce prompt length by 30-50%
- Use cached system prompts
- Batch similar extraction requests
- Consider using faster models for simple extractions (Claude Haiku vs Sonnet)

**E. PDF Processing**
- Implement progressive loading (send partial results)
- Pre-process common datasheets
- Use text extraction instead of OCR when possible
- Lazy load non-critical specs

### 2. Frontend Optimization

**A. Request Optimization**
```typescript
// Implement request debouncing (already partially done)
// Add request deduplication
const requestCache = new Map();

async function fetchWithDedup(key, fetchFn) {
  if (requestCache.has(key)) {
    return requestCache.get(key);
  }
  const promise = fetchFn();
  requestCache.set(key, promise);
  return promise;
}
```

**B. Progressive Loading**
```typescript
// Show results as they arrive
// Instead of waiting for all parts, stream them
async function* streamSearchResults(query) {
  const response = await fetch('/api/search/parts', {
    method: 'POST',
    body: JSON.stringify({ query, stream: true })
  });

  const reader = response.body.getReader();
  // Process stream chunks
}
```

**C. Optimize React Rendering**
```typescript
// Memoize expensive components
const SearchResultsContent = React.memo(({ responseText, query }) => {
  // Component implementation
});

// Use virtual scrolling for large result sets
import { VirtualScroll } from 'react-virtual';
```

### 3. Network Optimization

**A. Compression**
```typescript
// Enable gzip/brotli compression
// Backend: Add compression middleware
// Frontend: Already handled by fetch API

// Compress large JSON responses
```

**B. HTTP/2 Server Push**
```javascript
// Push critical resources
// - CSS
// - Initial data
```

**C. CDN for Static Assets**
- Host PDFs on CDN (CloudFront, Cloudflare)
- Cache datasheet URLs
- Use edge caching for search results

### 4. Database Optimization

**A. Query Optimization**
```sql
-- Analyze slow queries
EXPLAIN ANALYZE SELECT * FROM parts WHERE ...;

-- Add materialized views for common searches
CREATE MATERIALIZED VIEW popular_searches AS
SELECT query, results FROM search_cache
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY query;
```

**B. Connection Pooling**
```python
# Use connection pooling
from sqlalchemy import create_engine
engine = create_engine(
    DATABASE_URL,
    pool_size=20,
    max_overflow=10
)
```

---

## Expected Performance Improvements

| Optimization | Current | Target | Improvement |
|-------------|---------|--------|-------------|
| **Search API Call** | 10-30s | 2-5s | 5-10x faster |
| **PDF Processing** | 5-15s each | 1-3s each | 5x faster |
| **Spec Extraction** | 8-20s | 2-4s | 4-5x faster |
| **UI Rendering** | 100-300ms | 50-100ms | 2-3x faster |
| **Total Search** | 15-45s | 3-8s | 5-6x faster |

---

## Quick Wins (Implement First)

1. **Add Response Caching** (Backend)
   - Impact: 10-20x faster for repeated queries
   - Effort: Low (2-4 hours)
   - Implementation: Redis cache with 1-hour TTL

2. **Parallel PDF Processing** (Backend)
   - Impact: 3-5x faster for multi-part searches
   - Effort: Medium (4-8 hours)
   - Implementation: asyncio or multiprocessing

3. **Optimize AI Prompts** (Backend)
   - Impact: 30-50% faster AI calls
   - Effort: Low (1-2 hours)
   - Implementation: Reduce prompt tokens, use Haiku for simple tasks

4. **Add Request Deduplication** (Frontend)
   - Impact: Eliminates duplicate requests
   - Effort: Low (1 hour)
   - Implementation: Simple Map-based cache

5. **Enable Response Compression** (Backend)
   - Impact: 40-60% smaller payloads
   - Effort: Very Low (30 minutes)
   - Implementation: Add gzip middleware

---

## Monitoring Recommendations

### Backend Metrics to Track âœ… IMPLEMENTED

**Implementation Status:**
- âœ… Backend timing tracking added to `app/api/search/parts.py`
- Tracks: query generation, search engine, PDF processing
- Returns timing data in API response
- Logs timing breakdown to console

**Timing Data Tracked:**
```python
timing = {
    "total": 0,                    # Total request time
    "search_query_generation": 0,  # AI query generation
    "search_engine": 0,            # Search engine API call
    "pdf_processing": 0,           # PDF download + extraction
    "spec_extraction": 0           # Spec extraction from PDFs
}
```

**Console Output Example:**
```
â±ï¸  PERFORMANCE TIMING:
  Total: 12.45s
  - Query Generation: 1.23s
  - Search Engine: 2.34s
  - PDF Processing: 8.88s
```

**Additional Middleware (Optional):**
```python
# Add to backend for request-level timing
import time

@app.middleware("http")
async def add_timing_header(request, call_next):
    start = time.time()
    response = await call_next(request)
    duration = time.time() - start
    response.headers["X-Response-Time"] = str(duration)

    # Log slow requests
    if duration > 5.0:
        logger.warning(f"Slow request: {request.url} took {duration}s")

    return response
```

### Frontend Metrics
```typescript
// Add performance markers
performance.mark('search-start');
// ... perform search
performance.mark('search-end');
performance.measure('search-duration', 'search-start', 'search-end');

// Send to analytics
const measure = performance.getEntriesByName('search-duration')[0];
console.log(`Search took ${measure.duration}ms`);
```

### Set Performance Budgets
- **Search completion:** < 5 seconds (p50), < 10 seconds (p95)
- **Network requests:** < 3 seconds
- **PDF processing:** < 2 seconds per PDF
- **UI render:** < 100ms

---

## Testing Performance

### Load Testing Backend
```bash
# Use Apache Bench or k6
ab -n 100 -c 10 http://localhost:8000/api/search/parts

# Or k6 (more advanced)
k6 run load-test.js
```

### Profile Frontend
```javascript
// React DevTools Profiler
// Record session while searching
// Identify slow components

// Chrome DevTools Performance
// Record performance
// Check for long tasks (>50ms)
```

---

## Architecture Recommendations

### Short Term (1-2 weeks)
1. Add Redis caching layer
2. Implement parallel PDF processing
3. Optimize AI prompts
4. Add request deduplication

### Medium Term (1-2 months)
1. Implement progressive search results
2. Add database read replicas
3. Set up CDN for PDFs
4. Add result streaming

### Long Term (3-6 months)
1. Pre-process popular datasheets
2. Build search result prediction model
3. Implement edge caching
4. Add full-text search with Elasticsearch
5. Consider serverless functions for PDF processing

---

## Code Examples

### Backend Caching (Redis)
```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cache_search_result(query, result, ttl=3600):
    key = f"search:{hashlib.md5(query.encode()).hexdigest()}"
    redis_client.setex(key, ttl, json.dumps(result))

def get_cached_result(query):
    key = f"search:{hashlib.md5(query.encode()).hexdigest()}"
    cached = redis_client.get(key)
    return json.loads(cached) if cached else None
```

### Frontend Request Deduplication
```typescript
const pendingRequests = new Map<string, Promise<any>>();

async function fetchWithDedup<T>(
  key: string,
  fetchFn: () => Promise<T>
): Promise<T> {
  if (pendingRequests.has(key)) {
    return pendingRequests.get(key)!;
  }

  const promise = fetchFn().finally(() => {
    pendingRequests.delete(key);
  });

  pendingRequests.set(key, promise);
  return promise;
}

// Usage
const result = await fetchWithDedup(
  `search:${query}`,
  () => fetchColumnDeterminationAndSearch({ query })
);
```

---

## Next Steps

1. **Baseline Performance**
   - Run 10-20 searches
   - Record average times
   - Identify slowest operations

2. **Implement Quick Wins**
   - Start with caching
   - Add parallel processing
   - Optimize prompts

3. **Measure Improvements**
   - Compare before/after metrics
   - Target 5x improvement overall

4. **Iterate**
   - Profile remaining bottlenecks
   - Implement medium-term optimizations
   - Continue monitoring

---

## Support

For questions or assistance:
- Review timing logs in browser console
- Check Network tab in DevTools
- Monitor backend logs for slow operations
- Use the timing display in UI to identify frontend vs backend issues
